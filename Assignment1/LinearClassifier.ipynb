{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 : Training a simple Linear Model\n",
    "In this section, you will write the code to train a Linear Model. The goal is to classify an input $X_i$ of size $n$ into one of $m$ classes. For this, you need to consider the following:\n",
    "\n",
    "1)  **Weight Matrix** $W_{n\\times m}$: The Weights are multipled with the input $X_i$ (vector of size $n$), to find $m$ scores $S_m$ for the $m$ classes.\n",
    "\n",
    "2)  **The Loss function**:   \n",
    "  * The Cross Entropy Loss: By interpreting the scores as unnormalized log probabilities for each class, this loss tries to measure dissatisfaction with the scores in terms of the log probability of the right class:\n",
    "\n",
    "$$\n",
    "L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right) \\hspace{0.5in} \\text{or equivalently} \\hspace{0.5in} L_i = -f_{y_i} + \\log\\sum_j e^{f_j}\n",
    "$$\n",
    "\n",
    "where $f_{ y_i }$ is the $y_i$-th element of the output of $W^T  X_i$\n",
    "\n",
    "3) **A Regularization term**: In addition to the loss, you need a Regularization term to lead to a more distributed (in case of $L_2$) or sparse (in case of $L_1$) learning of the weights. For example, with $L_2$ regularization, the loss has the following additional term:\n",
    "\n",
    "$$\n",
    "R(W) = \\sum_k\\sum_l W_{k,l}^2  \n",
    "$$\n",
    "\n",
    "Thus the total loss has the form:\n",
    "$$\n",
    "L =  \\underbrace{ \\frac{1}{N} \\sum_i L_i }_\\text{data loss} + \\underbrace{ \\lambda R(W) }_\\text{regularization loss} \\\\\\\\\n",
    "$$\n",
    "\n",
    "4) **An Optimization Procedure**: This refers to the process which tweaks the weight Matrix $W_{n\\times m}$ to reduce the loss function $L$. In our case, this refers to Mini-batch Gradient Descent algorithm. We adjust the weights $W_{n\\times m}$, based on the gradient of the loss $L$ w.r.t. $W_{n\\times m}$. This leads to:\n",
    "$$\n",
    "W_{t+1} = W_{t} - \\alpha \\frac{\\partial L}{\\partial W},\n",
    "$$\n",
    "where $\\alpha$ is the learning rate. Additionally, with \"mini-batch\" gradient descent, instead of finding loss over the whole dataset, we use a small sample $B$ of the training data to make each learning step. Hence,\n",
    "$$\n",
    "W_{t+1} = W_{t} - \\alpha \\frac{\\partial \\sum_{i \\in B}{L_{x_i}}}{\\partial W},\n",
    "$$\n",
    "where $|B|$ is the batch size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('LogitAdjustmentLongTail')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49598912cb7efc65e0007e347a7051cc5ac4c91b95dad2ffbc631da6724968c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
