{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 : Training a simple Linear Model\n",
    "In this section, you will write the code to train a Linear Model. The goal is to classify an input $X_i$ of size $n$ into one of $m$ classes. For this, you need to consider the following:\n",
    "\n",
    "1)  **Weight Matrix** $W_{n\\times m}$: The Weights are multipled with the input $X_i$ (vector of size $n$), to find $m$ scores $S_m$ for the $m$ classes.\n",
    "\n",
    "2)  **The Loss function**:   \n",
    "  * The Cross Entropy Loss: By interpreting the scores as unnormalized log probabilities for each class, this loss tries to measure dissatisfaction with the scores in terms of the log probability of the right class:\n",
    "\n",
    "$$\n",
    "L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right) \\hspace{0.5in} \\text{or equivalently} \\hspace{0.5in} L_i = -f_{y_i} + \\log\\sum_j e^{f_j}\n",
    "$$\n",
    "\n",
    "where $f_{ y_i }$ is the $y_i$-th element of the output of $W^T  X_i$\n",
    "\n",
    "3) **A Regularization term**: In addition to the loss, you need a Regularization term to lead to a more distributed (in case of $L_2$) or sparse (in case of $L_1$) learning of the weights. For example, with $L_2$ regularization, the loss has the following additional term:\n",
    "\n",
    "$$\n",
    "R(W) = \\sum_k\\sum_l W_{k,l}^2  \n",
    "$$\n",
    "\n",
    "Thus the total loss has the form:\n",
    "$$\n",
    "L =  \\underbrace{ \\frac{1}{N} \\sum_i L_i }_\\text{data loss} + \\underbrace{ \\lambda R(W) }_\\text{regularization loss} \\\\\\\\\n",
    "$$\n",
    "\n",
    "4) **An Optimization Procedure**: This refers to the process which tweaks the weight Matrix $W_{n\\times m}$ to reduce the loss function $L$. In our case, this refers to Mini-batch Gradient Descent algorithm. We adjust the weights $W_{n\\times m}$, based on the gradient of the loss $L$ w.r.t. $W_{n\\times m}$. This leads to:\n",
    "$$\n",
    "W_{t+1} = W_{t} - \\alpha \\frac{\\partial L}{\\partial W},\n",
    "$$\n",
    "where $\\alpha$ is the learning rate. Additionally, with \"mini-batch\" gradient descent, instead of finding loss over the whole dataset, we use a small sample $B$ of the training data to make each learning step. Hence,\n",
    "$$\n",
    "W_{t+1} = W_{t} - \\alpha \\frac{\\partial \\sum_{i \\in B}{L_{x_i}}}{\\partial W},\n",
    "$$\n",
    "where $|B|$ is the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 1]\n",
    "Using the solution to ScalarBackProagation, construct a Linear Layer Class with a weight matrix W.\\\n",
    "The Class must have the capability to:\n",
    "- forward propagate given inputs\n",
    "- backpropagate to compute the gradients for the update of W i.e $$ \\frac{\\partial{L}}{\\partial{W_{i,j}}}$$\n",
    "- USE ONLY NUMPY OPERATIONS AND NOT ANY ML API for training and inference. (Data loading is fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now write the class func\n",
    "# which constructs the graph (all operators), forward and backward functions.\n",
    "\n",
    "class LinearLayer():\n",
    "  def __init__(self, args):\n",
    "    # construct the graph here\n",
    "    # assign the instances of function modules to self.var\n",
    "    pass\n",
    "    \n",
    "  def forward(self,x):\n",
    "    # Using the graph element's forward functions, get the output.\n",
    "    return output\n",
    "    \n",
    "  def backward(self, del_L_by_del_output):\n",
    "    # Use the saved outputs of each module, and backward() function calls\n",
    "    # updated the weight matrix\n",
    "    # clears the gradients for repeatable use\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from array import array\n",
    "\n",
    "class Loss():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    def forward(self, x) -> float:\n",
    "        pass\n",
    "    def backward(self) -> array:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the above Linear Layer, and loss function construct a Linear classifier for the CIFAR-10 dataset.\\\n",
    "- Convert the 3-d input (C, H, W) into a 1 d input by reshaping it.\\\n",
    "- You can use pytorch CIFAR-10 dataset but only as data loaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code below\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('LogitAdjustmentLongTail')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49598912cb7efc65e0007e347a7051cc5ac4c91b95dad2ffbc631da6724968c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
