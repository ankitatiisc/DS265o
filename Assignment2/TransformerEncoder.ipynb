{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Layers\n",
    "\n",
    "Given an input X, a (T, d) dimensional input write a function using numpy to feed it to an encoder and decoder layer of the transformer. You will also have to implement the internal workings of scaled dot-product Attention and Multi-head attention layers. For convenience consider the projection dimension to be the same (dim_size=d) for all Query, Key and Value. Only The forward propagation is expected to be implemented which will be executed by the forward method of each class.\n",
    "<center>\n",
    "<img src=\"./fig/Transformer.png\" width=\"324\" height=\"470\">\n",
    "</center>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"./fig/Attention-layers.png\" width=\"550\" height=\"350\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention:\n",
    "    def __init__(self, dim_size):\n",
    "        # declare weights here\n",
    "        pass\n",
    "    def forward(self, Q, K, V, Mask=None):\n",
    "        # returns the forward propagation output\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, dim_size, num_heads):\n",
    "        # declare the scaled dot product attention layers here\n",
    "        pass\n",
    "    def forward(self,  Q, K, V, Mask=None):\n",
    "        # returns the forward propagation output\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, dim_size, num_heads):\n",
    "        # declare your MHA layer\n",
    "        # weights for Feedforward layer\n",
    "        pass\n",
    "    def forward(self, X):\n",
    "        # returns the forward propagation output\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    def __init__(self, dim_size, num_heads1, num_heads2):\n",
    "        # declare your MHA layer\n",
    "        # weights for Feedforward layer\n",
    "        pass\n",
    "    def forward(self, X):\n",
    "        # returns the forward propagation output\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('LogitAdjustmentLongTail')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49598912cb7efc65e0007e347a7051cc5ac4c91b95dad2ffbc631da6724968c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
