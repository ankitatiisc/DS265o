{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Enoder Layer\n",
    "\n",
    "Given an input X, a (T, d) dimensional input write a function using numpy to feed it to an encoder layer of the transformer. You will also have to implement the internal workings of scaled dot-product Attention and Multi-head attention layers. For convenience consider the projection dimension to be the same (dim_size) for all Query, Key and Value. Only The forward propagation is expected to be implemented which will be executed by the forward method of each class.\n",
    "<center>\n",
    "<img src=\"./fig/Transformer.png\" width=\"324\" height=\"430\">\n",
    "\n",
    "<img src=\"./fig/Attention-layers.png\" width=\"325\" height=\"200\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention:\n",
    "    def __init__(self, dim_size):\n",
    "        # declare weights here\n",
    "    def forward(self, Q, K, V, Mask=None)\n",
    "        return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, dim_size, num_heads):\n",
    "        # declare the scaled dot product attention layers here\n",
    "    def forward(self,  Q, K, V, Mask=None)\n",
    "        return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, dim_size, num_heads):\n",
    "        # declare your MHA layer\n",
    "        # weights for Feedforward layer\n",
    "    def forward(self, X):\n",
    "        return Output"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
