{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Layers\n",
    "\n",
    "Given an input X, a (T, d) dimensional input write a function using numpy to feed it to an encoder and decoder layer of the transformer. You will also have to implement the internal workings of scaled dot-product Attention and Multi-head attention layers. For convenience consider the projection dimension to be the same (dim_size=d) for all Query, Key and Value. Only The forward propagation is expected to be implemented which will be executed by the forward method of each class.\n",
    "<center>\n",
    "<img src=\"./fig/Transformer.png\" width=\"324\" height=\"470\">\n",
    "</center>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"./fig/Attention-layers.png\" width=\"550\" height=\"350\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention:\n",
    "    def __init__(self, dim_size):\n",
    "        self.query_weights = np.random.randn(dim_size, dim_size)\n",
    "        self.query_bias = np.random.randn(dim_size)\n",
    "\n",
    "        self.key_weights = np.random.randn(dim_size, dim_size)\n",
    "        self.key_bias = np.random.randn(dim_size)\n",
    "\n",
    "        self.value_weights = np.random.randn(dim_size, dim_size)\n",
    "        self.value_bias = np.random.randn(dim_size)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # Compute dot product of queries and keys\n",
    "        dot_product = np.matmul(queries, keys.T)\n",
    "\n",
    "        # Divide dot product by square root of key dimension\n",
    "        dot_product /= np.sqrt(keys.shape[1])\n",
    "\n",
    "        # Apply softmax to the dot product to compute attention weights\n",
    "        attention_weights = np.exp(dot_product) / np.sum(np.exp(dot_product), axis=1, keepdims=True)\n",
    "\n",
    "        # Compute the weighted sum of the values using the attention weights\n",
    "        attention_outputs = np.matmul(attention_weights, values)\n",
    "\n",
    "        return attention_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, dim_size, num_heads):\n",
    "        # for each head, we shall create an SDPA (ScaledDotProductAttention)\n",
    "        # object that shall perform scaled dot roduct attention\n",
    "        self.SDPA_heads = [ScaledDotProductAttention(dim_size) for i in range(num_heads)]\n",
    "        self.linear_weights = np.random.randn(dim_size, dim_size)\n",
    "        self.linear_bias = np.random.randn(dim_size)\n",
    "\n",
    "    def forward(self,  Q, K, V, Mask=None):\n",
    "        # we will project the query, key and value based on \n",
    "        # W_q, W_k, W_v for each head\n",
    "        Q_heads = [np.matmul(Q, obj.query_weights) + obj.query_bias for obj in self.SDPA_heads]\n",
    "        K_heads = [np.matmul(K, obj.key_weights) + obj.key_bias for obj in self.SDPA_heads]\n",
    "        V_heads = [np.matmul(V, obj.value_weights) + obj.value_bias for obj in self.SDPA_heads]\n",
    "\n",
    "        # iteratively perform SDPA for each head and append the outputs\n",
    "        SDPA_outs = []\n",
    "        for obj, q, k, v in zip(self.SDPA_heads, Q_heads, K_heads, V_heads):\n",
    "            SDPA_outs.append(obj.forward(q, k, v))\n",
    "        \n",
    "        # concatenate the outputs of SDPA\n",
    "        # and reproject them using a linear function\n",
    "        SDPA_outputs_concatenated = np.concatenate(SDPA_outs, 1)\n",
    "        output = np.matmul(SDPA_outputs_concatenated, self.linear_weights) + \\\n",
    "                    self.linear_bias\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, dim_size, num_heads):\n",
    "        # declare your MHA layer\n",
    "        self.MHA = MultiHeadAttention(dim_size=dim_size, num_heads=num_heads)\n",
    "        # weights for Feedforward layer\n",
    "        self.linear_weights = np.random.randn(dim_size, dim_size)\n",
    "        self.linear_bias = np.random.randn(dim_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # returns the forward propagation output\n",
    "        MHA_out = self.MHA.forward(X, X, X)\n",
    "        mu1, sigma1 = np.mean(MHA_out, 1), np.sqrt(np.var(MHA_out,1))\n",
    "        sublayer1_out = (X + MHA_out - mu1)/sigma1\n",
    "        \n",
    "        # feed-forward and ReLU function\n",
    "        ff_out = np.maximum(0,np.matmul(sublayer1_out, self.linear_weights) + self.linear_bias)\n",
    "        sublayer2_out = ff_out + sublayer1_out\n",
    "\n",
    "        # compute the mean and sigma after the residual connection\n",
    "        mu2, sigma2 = np.mean(sublayer2_out, 1),\\\n",
    "                np.sqrt(np.var(sublayer2_out,1))\n",
    "        return (sublayer2_out - mu2)/sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    def __init__(self, dim_size, num_heads1, num_heads2):\n",
    "        # declare your MHA layer\n",
    "        self.MHA1 = MultiHeadAttention(dim_size=dim_size, num_heads=num_heads1)\n",
    "        self.MHA2 = MultiHeadAttention(dim_size=dim_size, num_heads=num_heads2)\n",
    "\n",
    "        # weights for Feedforward layer\n",
    "        self.linear_weights = np.random.randn(dim_size, dim_size)\n",
    "        self.linear_bias = np.random.randn(dim_size)\n",
    "\n",
    "    def forward(self, encoder_out, x):\n",
    "        MHA1_out = self.MHA1.forward(x, x, x) + x\n",
    "        mu1, sigma1 = np.mean(MHA1_out, 1), np.sqrt(np.var(MHA1_out,1))\n",
    "        MHA1_out = (MHA1_out - mu1)/sigma1\n",
    "\n",
    "        MHA2_out = self.MHA2.forward(encoder_out, encoder_out, MHA1_out) + MHA1_out\n",
    "        mu2, sigma2 = np.mean(MHA2_out, 1), np.sqrt(np.var(MHA2_out,1))\n",
    "        MHA2_out = (MHA2_out - mu2)/sigma2\n",
    "        \n",
    "        ff_out = np.maximum(0,np.matmul(MHA2_out, self.linear_weights) + self.linear_bias)\n",
    "        sublayer3_out = ff_out + MHA2_out\n",
    "\n",
    "        # compute the mean and sigma after the residual connection\n",
    "        mu3, sigma3 = np.mean(sublayer3_out, 1),\\\n",
    "                np.sqrt(np.var(sublayer3_out,1))\n",
    "        return (sublayer3_out - mu3)/sigma3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('LogitAdjustmentLongTail')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49598912cb7efc65e0007e347a7051cc5ac4c91b95dad2ffbc631da6724968c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
